@misc{xuan2025designevaluationgenerativeagentbased,
      title={Design and Evaluation of Generative Agent-based Platform for Human-Assistant Interaction Research: A Tale of 10 User Studies}, 
      author={Ziyi Xuan and Yiwen Wu and Xuhai Xu and Vinod Namboodiri and Mooi Choo Chuah and Yu Yang},
      year={2025},
      eprint={2505.09938},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2505.09938}, 
}

@inproceedings{10.1145/3715014.3724368,
author = {Xuan, Ziyi and Wu, Yiwen and Yang, Yu},
title = {Demo Abstract: GIDEA: Generative AI-Powered Interactive Design and Evaluation Platform for Assistant Agent Research},
year = {2025},
isbn = {9798400714795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715014.3724368},
doi = {10.1145/3715014.3724368},
abstract = {Conducting human-computer interaction (HCI) experiments often requires extensive manual effort, including configuring environments, recruiting participants, and recording interactions. We introduce GIDEA, a generative AI-powered interactive design and evaluation platform for assistant agents to streamline and accelerate HCI research. Our platform employs a three-role interaction pipeline, where researchers define experiments, large language model-driven avatars simulated participants, and a smart assistant agent moderates interactions. This pipeline dynamically generates interaction scenarios, avatar profiles, and adaptive responses based on researcher input. By integrating with Unity, GIDEA enables real-time monitoring and control over simulated experiments, providing researchers with an interactive and adaptable evaluation environment. Through the replication of real-world case studies, we demonstrate that GIDEA reduces the time and effort required for HCI experiments while producing results that align with real studies. This capability has the potential to revolutionize HCI research by transforming traditionally lengthy and labor-intensive processes into a highly efficient, scalable, and adaptive methodology, accelerating innovation and broadening experimental possibilities.},
booktitle = {Proceedings of the 23rd ACM Conference on Embedded Networked Sensor Systems},
pages = {704–705},
numpages = {2},
keywords = {assistant agent, user simulation, simulation-based experimentation, large language models, human-computer interaction},
location = {UC Irvine Student Center., Irvine, CA, USA},
series = {SenSys '25}
}

@inproceedings{10.1145/3715014.3722065,
author = {Nie, Jingping and Fan, Yuang and Zhao, Minghui and Wan, Runxi and Xuan, Ziyi and Preindl, Matthias and Jiang, Xiaofan},
title = {Multi-Modal Dataset Across Exertion Levels: Capturing Post-Exercise Speech, Breathing, and Phonocardiogram},
year = {2025},
isbn = {9798400714795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715014.3722065},
doi = {10.1145/3715014.3722065},
abstract = {Cardio exercise elevates both heart rate and respiration rate, resulting in distinct physiological changes that affect speech patterns, pitch, breathing sounds, and heart sounds. These variations, which occur post-exercise, are influenced by factors such as exercise intensity and individual fitness levels. A comprehensive audio dataset is critically needed to capture post-exercise physiological changes, as existing datasets focus mainly on resting speech, breathing, and heart sounds, neglecting the dynamic shifts following physical exertion. Current datasets fail to capture unique post-exercise variations like speech disfluencies, altered breathing patterns, and variable heart sound intensities, limiting model generalizability to post-exercise conditions. To address this gap, we recruited 59 subjects from diverse backgrounds to engage in cardio exercise, specifically running, reaching varied exertion levels to produce a rich dataset. Our dataset includes 250 sessions totaling 143 minutes of structured reading, 47 minutes of spontaneous speech, 71 minutes of breathing sounds, and 62.5 minutes of phonocardiogram (PCG) recordings. We designed and deployed preliminary case studies to show that speech changes post-cardio could serve as an indicator of exertion level. We envision this dataset as a foundational resource for designing models in speech and cardiorespiratory monitoring that are resilient to the physiological shifts induced by exercise. This dataset could advance natural language processing (NLP) applications, mobile health, and wearable sensing technologies by enabling resilient and accurate physiological monitoring in real-world conditions.},
booktitle = {Proceedings of the 23rd ACM Conference on Embedded Networked Sensor Systems},
pages = {297–304},
numpages = {8},
keywords = {speech, biosignals, mobile health, human-centered computing, multimodal data},
location = {UC Irvine Student Center., Irvine, CA, USA},
series = {SenSys '25}
}

@inproceedings{10.1145/3636534.3697446,
author = {Nie, Jingping and Fan, Yuang and Xuan, Ziyi and Preindl, Matthias and Jiang, Xiaofan},
title = {Real-Time Non-Contact Estimation of Running Metrics on Treadmills using Smartphones},
year = {2024},
isbn = {9798400704895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636534.3697446},
doi = {10.1145/3636534.3697446},
abstract = {Over half a trillion recreational runners worldwide engage in running for psychological, health, and social benefits. Running metrics are essential for motivation, goal setting, performance improvement, health management, and injury prevention. Although wearable devices like fitness trackers and smartwatches offer various metrics, they often perform poorly on treadmills and can be uncomfortable or restrictive. In this work, we propose a non-contact, real-time, smartphone-based approach to estimate running metrics, including cadence, ground contact time (GCT), and balance, using the sound produced during treadmill running. In collaboration with a licensed running coach, we recruited over 50 subjects with varying levels of running expertise. We collected treadmill running sounds and ground-truth running metrics in different environments. We designed and developed a multi-task learning (MTL) machine learning mobile system to capture the treadmill running sounds and estimate running metrics in situ. Our proposed method shows comparable accuracy in estimating running metrics to commercial off-the-shelf (COTS) wearable devices.},
booktitle = {Proceedings of the 30th Annual International Conference on Mobile Computing and Networking},
pages = {1644–1646},
numpages = {3},
keywords = {mobile computing, machine learning, acoustics, mobile application for health and wellness},
location = {Washington D.C., DC, USA},
series = {ACM MobiCom '24}
}

@inproceedings{10.1145/3576914.3589561,
author = {Xuan, Ziyi and Liu, Ming and Nie, Jingping and Zhao, Minghui and Xia, Stephen and Jiang, Xiaofan},
title = {CaNRun: Non-Contact, Acoustic-based Cadence Estimation on Treadmills using Smartphones},
year = {2023},
isbn = {9798400700491},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576914.3589561},
doi = {10.1145/3576914.3589561},
abstract = {Running with a consistent cadence (number of steps per minute) is important for runners to help reduce risk of injury, improve running form, and enhance overall bio-mechanical efficiency. We introduce CaNRun, a non-contact and acoustic-based system that uses sound captured from a mobile device placed on a treadmill to predict and report running cadence. CaNRun obviates the need for runners to utilize wearable devices or carry a mobile device on their body while running on a treadmill. CaNRun leverages a long short-term memory (LSTM) network to extract steps observed from the microphone to robustly estimate cadence. Through an 8-person study, we demonstrate that CaNRun achieves cadence detection accuracy without calibration for individual users, which is comparable to the accuracy of the Apple Watch despite being non-contact.},
booktitle = {Proceedings of Cyber-Physical Systems and Internet of Things Week 2023},
pages = {272–277},
numpages = {6},
keywords = {Acoustic Sensing, Cadence Estimation, Embedded Systems},
location = {San Antonio, TX, USA},
series = {CPS-IoT Week '23}
}